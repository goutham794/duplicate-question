{
  
    
        "post0": {
            "title": "Duplicate Question Detection using LSTMs in PyTorch.",
            "content": "Notebook written to work in Google Colab . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . %cd /content/drive/My Drive/quora-question-pairs/ . /content/drive/My Drive/quora-question-pairs . from torchtext.data import Field, TabularDataset, BucketIterator, Example, Iterator import torchtext.vocab as vocab import torch import torch.nn as nn import torch.optim as optim from torch.utils.tensorboard import SummaryWriter # from tqdm import tqdm from tqdm.notebook import tqdm import re import numpy as np import pandas as pd from sklearn import model_selection import matplotlib.pyplot as plt %matplotlib inline from layers import RNNDropout . torch.manual_seed(0) np.random.seed(0) . BATCH_SIZE = 512 . device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;) . data = pd.read_csv(&quot;input/train.csv&quot;, nrows=20000 ) . data.head() . id qid1 qid2 question1 question2 is_duplicate . 0 0 | 1 | 2 | What is the step by step guide to invest in sh... | What is the step by step guide to invest in sh... | 0 | . 1 1 | 3 | 4 | What is the story of Kohinoor (Koh-i-Noor) Dia... | What would happen if the Indian government sto... | 0 | . 2 2 | 5 | 6 | How can I increase the speed of my internet co... | How can Internet speed be increased by hacking... | 0 | . 3 3 | 7 | 8 | Why am I mentally very lonely? How can I solve... | Find the remainder when [math]23^{24}[/math] i... | 0 | . 4 4 | 9 | 10 | Which one dissolve in water quikly sugar, salt... | Which fish would survive in salt water? | 0 | . Cleaning the text . contraction_mapping = {&quot;ain&#39;t&quot;: &quot;is not&quot;, &quot;aren&#39;t&quot;: &quot;are not&quot;,&quot;can&#39;t&quot;: &quot;cannot&quot;, &quot;&#39;cause&quot;: &quot;because&quot;, &quot;could&#39;ve&quot;: &quot;could have&quot;, &quot;couldn&#39;t&quot;: &quot;could not&quot;, &quot;didn&#39;t&quot;: &quot;did not&quot;, &quot;doesn&#39;t&quot;: &quot;does not&quot;, &quot;don&#39;t&quot;: &quot;do not&quot;, &quot;hadn&#39;t&quot;: &quot;had not&quot;, &quot;hasn&#39;t&quot;: &quot;has not&quot;, &quot;haven&#39;t&quot;: &quot;have not&quot;, &quot;he&#39;d&quot;: &quot;he would&quot;,&quot;he&#39;ll&quot;: &quot;he will&quot;, &quot;he&#39;s&quot;: &quot;he is&quot;, &quot;how&#39;d&quot;: &quot;how did&quot;, &quot;how&#39;d&#39;y&quot;: &quot;how do you&quot;, &quot;how&#39;ll&quot;: &quot;how will&quot;, &quot;how&#39;s&quot;: &quot;how is&quot;, &quot;I&#39;d&quot;: &quot;I would&quot;, &quot;I&#39;d&#39;ve&quot;: &quot;I would have&quot;, &quot;I&#39;ll&quot;: &quot;I will&quot;, &quot;I&#39;ll&#39;ve&quot;: &quot;I will have&quot;,&quot;I&#39;m&quot;: &quot;I am&quot;, &quot;I&#39;ve&quot;: &quot;I have&quot;, &quot;i&#39;d&quot;: &quot;i would&quot;, &quot;i&#39;d&#39;ve&quot;: &quot;i would have&quot;, &quot;i&#39;ll&quot;: &quot;i will&quot;, &quot;i&#39;ll&#39;ve&quot;: &quot;i will have&quot;,&quot;i&#39;m&quot;: &quot;i am&quot;,&#39;i &#39;m&#39;:&#39;i am&#39;, &quot;i&#39;ve&quot;: &quot;i have&quot;, &quot;isn&#39;t&quot;: &quot;is not&quot;, &quot;it&#39;d&quot;: &quot;it would&quot;, &quot;it&#39;d&#39;ve&quot;: &quot;it would have&quot;, &quot;it&#39;ll&quot;: &quot;it will&quot;, &quot;it&#39;ll&#39;ve&quot;: &quot;it will have&quot;,&quot;it&#39;s&quot;: &quot;it is&quot;, &quot;let&#39;s&quot;: &quot;let us&quot;, &quot;ma&#39;am&quot;: &quot;madam&quot;, &quot;mayn&#39;t&quot;: &quot;may not&quot;, &quot;might&#39;ve&quot;: &quot;might have&quot;,&quot;mightn&#39;t&quot;: &quot;might not&quot;,&quot;mightn&#39;t&#39;ve&quot;: &quot;might not have&quot;, &quot;must&#39;ve&quot;: &quot;must have&quot;, &quot;mustn&#39;t&quot;: &quot;must not&quot;, &quot;mustn&#39;t&#39;ve&quot;: &quot;must not have&quot;, &quot;needn&#39;t&quot;: &quot;need not&quot;, &quot;needn&#39;t&#39;ve&quot;: &quot;need not have&quot;,&quot;o&#39;clock&quot;: &quot;of the clock&quot;, &quot;oughtn&#39;t&quot;: &quot;ought not&quot;, &quot;oughtn&#39;t&#39;ve&quot;: &quot;ought not have&quot;, &quot;shan&#39;t&quot;: &quot;shall not&quot;, &quot;sha&#39;n&#39;t&quot;: &quot;shall not&quot;, &quot;shan&#39;t&#39;ve&quot;: &quot;shall not have&quot;, &quot;she&#39;d&quot;: &quot;she would&quot;, &quot;she&#39;d&#39;ve&quot;: &quot;she would have&quot;, &quot;she&#39;ll&quot;: &quot;she will&quot;, &quot;she&#39;ll&#39;ve&quot;: &quot;she will have&quot;, &quot;she&#39;s&quot;: &quot;she is&quot;, &quot;should&#39;ve&quot;: &quot;should have&quot;, &quot;shouldn&#39;t&quot;: &quot;should not&quot;, &quot;shouldn&#39;t&#39;ve&quot;: &quot;should not have&quot;, &quot;so&#39;ve&quot;: &quot;so have&quot;,&quot;so&#39;s&quot;: &quot;so as&quot;, &quot;this&#39;s&quot;: &quot;this is&quot;,&quot;that&#39;d&quot;: &quot;that would&quot;, &quot;that&#39;d&#39;ve&quot;: &quot;that would have&quot;, &quot;that&#39;s&quot;: &quot;that is&quot;, &quot;there&#39;d&quot;: &quot;there would&quot;, &quot;there&#39;d&#39;ve&quot;: &quot;there would have&quot;, &quot;there&#39;s&quot;: &quot;there is&quot;, &quot;here&#39;s&quot;: &quot;here is&quot;,&quot;they&#39;d&quot;: &quot;they would&quot;, &quot;they&#39;d&#39;ve&quot;: &quot;they would have&quot;, &quot;they&#39;ll&quot;: &quot;they will&quot;, &quot;they&#39;ll&#39;ve&quot;: &quot;they will have&quot;, &quot;they&#39;re&quot;: &quot;they are&quot;, &quot;they&#39;ve&quot;: &quot;they have&quot;, &quot;to&#39;ve&quot;: &quot;to have&quot;, &quot;wasn&#39;t&quot;: &quot;was not&quot;, &quot;we&#39;d&quot;: &quot;we would&quot;, &quot;we&#39;d&#39;ve&quot;: &quot;we would have&quot;, &quot;we&#39;ll&quot;: &quot;we will&quot;, &quot;we&#39;ll&#39;ve&quot;: &quot;we will have&quot;, &quot;we&#39;re&quot;: &quot;we are&quot;, &quot;we&#39;ve&quot;: &quot;we have&quot;, &quot;weren&#39;t&quot;: &quot;were not&quot;, &quot;what&#39;ll&quot;: &quot;what will&quot;, &quot;what&#39;ll&#39;ve&quot;: &quot;what will have&quot;, &quot;what&#39;re&quot;: &quot;what are&quot;, &quot;what&#39;s&quot;: &quot;what is&quot;, &quot;what&#39;ve&quot;: &quot;what have&quot;, &quot;when&#39;s&quot;: &quot;when is&quot;, &quot;when&#39;ve&quot;: &quot;when have&quot;, &quot;where&#39;d&quot;: &quot;where did&quot;, &quot;where&#39;s&quot;: &quot;where is&quot;, &quot;where&#39;ve&quot;: &quot;where have&quot;, &quot;who&#39;ll&quot;: &quot;who will&quot;, &quot;who&#39;ll&#39;ve&quot;: &quot;who will have&quot;, &quot;who&#39;s&quot;: &quot;who is&quot;, &quot;who&#39;ve&quot;: &quot;who have&quot;, &quot;why&#39;s&quot;: &quot;why is&quot;, &quot;why&#39;ve&quot;: &quot;why have&quot;, &quot;will&#39;ve&quot;: &quot;will have&quot;, &quot;won&#39;t&quot;: &quot;will not&quot;, &quot;won&#39;t&#39;ve&quot;: &quot;will not have&quot;, &quot;would&#39;ve&quot;: &quot;would have&quot;, &quot;wouldn&#39;t&quot;: &quot;would not&quot;, &quot;wouldn&#39;t&#39;ve&quot;: &quot;would not have&quot;, &quot;y&#39;all&quot;: &quot;you all&quot;, &quot;y&#39;all&#39;d&quot;: &quot;you all would&quot;,&quot;y&#39;all&#39;d&#39;ve&quot;: &quot;you all would have&quot;,&quot;y&#39;all&#39;re&quot;: &quot;you all are&quot;,&quot;y&#39;all&#39;ve&quot;: &quot;you all have&quot;,&quot;you&#39;d&quot;: &quot;you would&quot;, &quot;you&#39;d&#39;ve&quot;: &quot;you would have&quot;, &quot;you&#39;ll&quot;: &quot;you will&quot;, &quot;you&#39;ll&#39;ve&quot;: &quot;you will have&quot;, &quot;you&#39;re&quot;: &quot;you are&quot;, &quot;you&#39;ve&quot;: &quot;you have&quot;, &#39;colour&#39;: &#39;color&#39;, &#39;centre&#39;: &#39;center&#39;, &#39;favourite&#39;: &#39;favorite&#39;, &#39;travelling&#39;: &#39;traveling&#39;, &#39;counselling&#39;: &#39;counseling&#39;, &#39;theatre&#39;: &#39;theater&#39;, &#39;cancelled&#39;: &#39;canceled&#39;} def clean_contractions(text, mapping): text = text.lower() specials = [&quot;’&quot;, &quot;‘&quot;, &quot;´&quot;, &quot;`&quot;] for s in specials: text = text.replace(s, &quot;&#39;&quot;) text = &#39; &#39;.join([mapping[t] if t in mapping else mapping[t.lower()] if t.lower() in mapping else t for t in text.split(&quot; &quot;)]) return text def remove_newlines(sent): sent = re.sub(r&#39; s+&#39;, &quot; &quot;, sent ) return sent data[&#39;question1&#39;] = data[&#39;question1&#39;].apply(lambda x: clean_contractions(str(x),contraction_mapping)) data[&#39;question2&#39;] = data[&#39;question2&#39;].apply(lambda x: clean_contractions(str(x),contraction_mapping)) data[&#39;question1&#39;] = data[&#39;question1&#39;].apply(lambda x: remove_newlines(str(x))) data[&#39;question2&#39;] = data[&#39;question2&#39;].apply(lambda x: remove_newlines(str(x))) . data.dropna(subset = [&quot;question1&quot;, &quot;question2&quot;], inplace=True) . data.shape . (20000, 6) . data.head() . id qid1 qid2 question1 question2 is_duplicate . 0 0 | 1 | 2 | what is the step by step guide to invest in sh... | what is the step by step guide to invest in sh... | 0 | . 1 1 | 3 | 4 | what is the story of kohinoor (koh-i-noor) dia... | what would happen if the indian government sto... | 0 | . 2 2 | 5 | 6 | how can i increase the speed of my internet co... | how can internet speed be increased by hacking... | 0 | . 3 3 | 7 | 8 | why am i mentally very lonely? how can i solve... | find the remainder when [math]23^{24}[/math] i... | 0 | . 4 4 | 9 | 10 | which one dissolve in water quikly sugar, salt... | which fish would survive in salt water? | 0 | . train, valid = model_selection.train_test_split( data, test_size=0.1, random_state=42, stratify=data.is_duplicate.values ) . train.to_csv(&#39;input/train_split.csv&#39;,index = False) valid.to_csv(&#39;input/validation_split.csv&#39;,index = False) . Torchtext&#39;s Field object can be used to tokenize, numericalize text. . question_1 = Field(sequential=True, use_vocab=True, tokenize=&#39;spacy&#39;, batch_first=True, lower=True) question_2 = Field(sequential=True, use_vocab=True, tokenize=&#39;spacy&#39;, batch_first=True, lower=True) target = Field(sequential=False, use_vocab=False) # Specifying which columns to use from Dataset, and which Field should be used to process each column. fields = {&#39;question1&#39; : (&#39;q1&#39;,question_1), &#39;question2&#39; : (&#39;q2&#39;,question_2), &#39;is_duplicate&#39; : (&#39;t&#39;, target)} . train_data, valid_data = TabularDataset.splits( path=&#39;input/&#39;, train=&#39;train_split.csv&#39;, test=&#39;validation_split.csv&#39;, format=&#39;csv&#39;, fields=fields) . Vocab is built separately for Question 1 and Question 2 and then combined. . Glove is used for embedding layer. . question_1.build_vocab(train_data, max_size=200000) question_2.build_vocab(train_data, max_size=200000) question_1.vocab.extend(question_2.vocab) question_2.vocab = question_1.vocab glove_embeddings = vocab.Vectors(name = &#39;/content/drive/My Drive/glove/glove.6B.300d.txt&#39;) question_1.vocab.load_vectors(glove_embeddings) . BucketIterator serves the purpose of dataloader, can be used to get batches of sentences with similar length so that padding is minimized. . train_iterator, valid_iterator = BucketIterator.splits( (train_data, valid_data), batch_size=BATCH_SIZE, device=device, sort_key = lambda x: len(x.q1)+len(x.q2), sort_within_batch=True ) . Model . class Net(nn.Module): # defining the structure of the network def __init__(self, embedding_weights, embed_size, hidden_size, num_layers,dropout): super().__init__() self.hidden_size = hidden_size self.num_layers = num_layers self.embedding = nn.Embedding.from_pretrained( embeddings=embedding_weights, freeze=True, padding_idx=question_1.vocab.stoi[&quot;&lt;pad&gt;&quot;], ) self.rnn_dropout = RNNDropout(p=0.3) self.rnn1 = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True, dropout=0.3, bidirectional=True) # self.rnn2 = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True) self.bn = nn.BatchNorm1d(2*hidden_size) self.fc = nn.Sequential( nn.Linear(4*hidden_size, hidden_size), nn.ReLU(), nn.Dropout(dropout), nn.Linear(self.hidden_size, self.hidden_size // 2), nn.ReLU(), nn.Dropout(dropout), nn.Linear(hidden_size // 2, 1) ) # defining steps in forward pass def forward(self, x1, x2): embedded_q1 = self.embedding(x1) embedded_q2 = self.embedding(x2) embedded_q1 = self.rnn_dropout(embedded_q1) embedded_q2 = self.rnn_dropout(embedded_q2) # output of the LSTM - shape (seq_len, batch, num_directions * hidden_size): tensor containing the output features (h_t) from the last layer of the LSTM, for each t. # We are only interested in the hidden state at the last time step. outputs_q1, _ = self.rnn1(embedded_q1) outputs_q2, _ = self.rnn1(embedded_q2) output_1 = outputs_q1[:, -1, :] output_2 = outputs_q2[:, -1, :] concatenated_output = torch.cat((output_1, output_2), dim=1) # concatenated_output = self.bn(concatenated_output) prediction = self.fc(concatenated_output) return prediction . hidden_size = 256 num_layers = 2 embedding_size = 300 learning_rate = 0.003 num_epochs = 10 dropout = 0.5 max_gradient_norm=10.0 . model = Net(question_1.vocab.vectors, embedding_size, hidden_size, num_layers, dropout).to(device) . . 149263/255027 . 0.5852831268846044 . 149263/(149263+255027) . 0.369197853026293 . Class weight is used as classes are imbalanced. . class_weight = torch.FloatTensor([0.5852831268846044]).to(device) . criterion = nn.BCEWithLogitsLoss(pos_weight=class_weight) optimizer = optim.Adam([param for param in model.parameters() if param.requires_grad == True], lr=learning_rate) . writer = SummaryWriter(f&#39;runs/LSTM-Dropout-0.3-10&#39;) . def accuracy(logits,label): sigmoid = nn.Sigmoid()(logits) predictions = torch.round(sigmoid) predictions = predictions.view(logits.shape[0]) return (predictions == label).sum().float()/float(label.size(0)) . def train_fn(data_loader, model, optimizer, device): model.train() epoch_train_accuracy = 0 epoch_train_loss = 0 for batch_idx, batch in tqdm(enumerate(data_loader), total=len(data_loader)): q1 = batch.q1.to(device=device) q2 = batch.q2.to(device=device) targets = batch.t.to(device=device) predictions = model(q1, q2) loss = criterion(predictions.squeeze(1), targets.type_as(predictions)) optimizer.zero_grad() # backward loss.backward() nn.utils.clip_grad_norm_(model.parameters(), max_gradient_norm) optimizer.step() batch_accuracy = accuracy(predictions, targets) epoch_train_accuracy += batch_accuracy.item() epoch_train_loss += loss.item() return epoch_train_loss/len(data_loader), epoch_train_accuracy/len(data_loader) . def eval_fn(data_loader, model, device): model.eval() epoch_valid_accuracy = 0 epoch_valid_loss = 0 with torch.no_grad(): for batch_idx, batch in tqdm(enumerate(data_loader), total=len(data_loader)): q1 = batch.q1.to(device=device) q2 = batch.q2.to(device=device) targets = batch.t.to(device=device) predictions = model(q1, q2) loss = criterion(predictions.squeeze(1), targets.type_as(predictions)) batch_accuracy = accuracy(predictions, targets) epoch_valid_accuracy += batch_accuracy.item() epoch_valid_loss += loss.item() return epoch_valid_loss/len(data_loader), epoch_valid_accuracy/len(data_loader) . train_loss_values = [] valid_loss_values = [] train_accuracies = [] valid_accuracies = [] best_loss = 1000 step = 0 for epoch in range(num_epochs): train_loss, train_accuracy = train_fn(train_iterator, model, optimizer, device) writer.add_scalar(&#39;Train Epoch loss&#39;, train_loss, global_step=step) writer.add_scalar(&#39;Train Epoch Accuracy&#39;, train_accuracy, global_step=step) train_loss_values.append(train_loss) train_accuracies.append(train_accuracy) valid_loss, valid_accuracy = eval_fn(valid_iterator, model, device) writer.add_scalar(&#39;Valid Epoch loss&#39;, valid_loss, global_step=step) writer.add_scalar(&#39;Valid Epoch Accuracy&#39;, valid_accuracy, global_step=step) valid_loss_values.append(valid_loss) valid_accuracies.append(valid_accuracy) print(f&quot;Epoch {epoch} Train loss - {train_loss} Train accuracy - {train_accuracy} Valid loss - {valid_loss} Valid accuracy - {valid_accuracy}&quot;) step+=1 if valid_loss &lt; best_loss: torch.save(model.state_dict(), &quot;models/LSTM-GLOVE-300.bin&quot;) best_loss = valid_loss . Epoch 0 Train loss - 0.4280065979635665 Train accuracy - 0.6834836923288058 Valid loss - 0.3922902002364774 Valid accuracy - 0.7499114878569977 Epoch 1 Train loss - 0.3765430258883035 Train accuracy - 0.7383813668571612 Valid loss - 0.36230450698846506 Valid accuracy - 0.7693714273126819 Epoch 2 Train loss - 0.3574556076870354 Train accuracy - 0.7548057713589085 Valid loss - 0.34458604172060764 Valid accuracy - 0.7640312374392643 Epoch 3 Train loss - 0.34202399428635205 Train accuracy - 0.7675752168633935 Valid loss - 0.3426209559923486 Valid accuracy - 0.7716449997093104 Epoch 4 Train loss - 0.3295811785885386 Train accuracy - 0.7794121367183583 Valid loss - 0.3256099358012405 Valid accuracy - 0.7845514114898972 Epoch 5 Train loss - 0.31693398095300596 Train accuracy - 0.7895898214540066 Valid loss - 0.31646565866621235 Valid accuracy - 0.7985941330088845 Epoch 6 Train loss - 0.3054027983869011 Train accuracy - 0.7979503138994198 Valid loss - 0.3119266927242279 Valid accuracy - 0.8000033497810364 Epoch 7 Train loss - 0.2969776293585069 Train accuracy - 0.804117386434987 Valid loss - 0.3135194619999656 Valid accuracy - 0.8031679067430617 Epoch 8 Train loss - 0.2895943476136056 Train accuracy - 0.8097377150370602 Valid loss - 0.3188745711423174 Valid accuracy - 0.8083597580088845 Epoch 9 Train loss - 0.2844139492838695 Train accuracy - 0.8148993572269981 Valid loss - 0.3142248908930187 Valid accuracy - 0.8129088086417958 . fig = plt.figure(dpi=200) plt.ylabel(&quot;Loss&quot;) plt.plot(np.array(train_loss_values), &#39;r&#39;) plt.plot(np.array(valid_loss_values), &#39;y&#39;) plt.legend([&quot;Train Loss&quot;, &quot;Validation loss&quot;], loc =&quot;upper right&quot;) . &lt;matplotlib.legend.Legend at 0x7f18c47bd4a8&gt; . Test data . test_fields = {&#39;question1&#39; : (&#39;q1&#39;,question_1), &#39;question2&#39; : (&#39;q2&#39;,question_2)} . test = pd.read_csv(&quot;input/test.csv&quot;) . test[&#39;question1&#39;] = test[&#39;question1&#39;].apply(lambda x: clean_contractions(str(x),contraction_mapping)) test[&#39;question2&#39;] = test[&#39;question2&#39;].apply(lambda x: clean_contractions(str(x),contraction_mapping)) test[&#39;question1&#39;] = test[&#39;question1&#39;].apply(lambda x: remove_newlines(str(x))) test[&#39;question2&#39;] = test[&#39;question2&#39;].apply(lambda x: remove_newlines(str(x))) . test.head() . test_id question1 question2 . 0 0 | how does the surface pro himself 4 compare wit... | why did microsoft choose core m3 and not core ... | . 1 1 | should i have a hair transplant at age 24? how... | how much cost does hair transplant require? | . 2 2 | what but is the best way to send money from ch... | what you send money to china? | . 3 3 | which food not emulsifiers? | what foods fibre? | . 4 4 | how &quot;aberystwyth&quot; start reading? | how their can i start reading? | . test.to_csv(&#39;input/test_split.csv&#39;,index = False) . test_data = TabularDataset( path=&#39;input/test_split.csv&#39;, format=&#39;CSV&#39;, fields=test_fields ) . test_iterator = BucketIterator( test_data, batch_size=32, device=device,train=False,sort=False) . model = Net(question_1.vocab.vectors, embedding_size, hidden_size, num_layers, dropout).to(device) model.load_state_dict(torch.load(&quot;models/LSTM-GLOVE-300.bin&quot;)) . &lt;All keys matched successfully&gt; . def predict(data_loader, model, device): model.eval() all_predictions = [] with torch.no_grad(): for batch_idx, batch in tqdm(enumerate(data_loader), total=len(data_loader)): q1 = batch.q1.to(device=device) q2 = batch.q2.to(device=device) predictions = model(q1, q2) predictions = nn.Sigmoid()(predictions).cpu().tolist() all_predictions += predictions return all_predictions . test_predictions = predict(test_iterator, model, device) . . test_predictions = [item[0] for item in test_predictions] . test[&#39;is_duplicate&#39;] = test_predictions . test_predictions[0] . 0.08426652103662491 . test.to_csv(&#39;submission_lstm.csv&#39;, columns=[&#39;test_id&#39;,&#39;is_duplicate&#39;], index=False) . !ls . input lstm-code.py runs submission.csv use_model input_maLSTM models submission_1.csv train.csv validation.csv . test.head() . test_id question1 question2 is_duplicate . 0 0 | How does the Surface Pro himself 4 compare wit... | Why did Microsoft choose core m3 and not core ... | 0.137905 | . 1 1 | Should I have a hair transplant at age 24? How... | How much cost does hair transplant require? | 0.035996 | . 2 2 | What but is the best way to send money from Ch... | What you send money to China? | 0.020188 | . 3 3 | Which food not emulsifiers? | What foods fibre? | 0.030641 | . 4 4 | How &quot;aberystwyth&quot; start reading? | How their can I start reading? | 0.039753 | . - BiLSTM - Swap Q1, Q2 - Mnahattan LSTM - LSTM shared weights - Common Embedding layer .",
            "url": "https://goutham794.github.io/duplicate-question/jupyter/2021/03/06/duplicate-question-detection.html",
            "relUrl": "/jupyter/2021/03/06/duplicate-question-detection.html",
            "date": " • Mar 6, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://goutham794.github.io/duplicate-question/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://goutham794.github.io/duplicate-question/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://goutham794.github.io/duplicate-question/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://goutham794.github.io/duplicate-question/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}